include "std"

const Tokens.Word      1 offset end
const TOKEN_KIND_COUNT reset end

const Token.kind  sizeof(u8) offset end
const Token.ptr   sizeof(ptr) offset end
const Token.int   sizeof(int) offset end
const sizeof(Token) reset end

const TOKENS_CAP 4096 end

memory tokens      TOKENS_CAP sizeof(Token) mul end
memory token-count sizeof(int) end

inline proc @Token :: ptr -> int ptr int do
  let ptr do
    ptr Token.kind ptr+ read8
    ptr Token.ptr  ptr+ read64 cast(ptr)
    ptr Token.int  ptr+ read64
  end
end

proc format-token-kind :: int -> int ptr do
  assert "Exhaustive token handling in format-token-kind" TOKEN_KIND_COUNT 1 eq end
  dup Tokens.Word eq if
    "Word"
  else
    "Unknown"
  end
  rot drop
end

inline proc is-whitespace :: int -> bool do
  dup  ' '  eq
  swap '\n' eq
  lor
end

proc Lexer.skip-whitespace :: int ptr int -> int do
  let len str inc do
    inc while
      dup len lt if
        str over ptr+ read8 is-whitespace
      else false end
    do 1 add end
  end
end

proc Lexer.skip-word :: int ptr int -> int do
  let len str inc do
    inc while
      dup len lt if
        str over ptr+ read8 is-whitespace lnot
      else false end
    do 1 add end
  end
end

proc Lexer.push-token :: int ptr int do
  token-count read64
  dup TOKENS_CAP gteq if
    "ERROR: Reached maximum amount of tokens (" puts TOKENS_CAP putu ")\n" puts
    1 exit
  end

  tokens over sizeof(Token) mul ptr+
  let kind data-ptr data-int count ptr do
    kind               ptr Token.kind ptr+ write8
    data-ptr cast(int) ptr Token.ptr  ptr+ write64
    data-int           ptr Token.int  ptr+ write64

    count 1 add token-count write64
  end
end

proc Lexer.tokenize :: int ptr do
  let len str do
    0 while dup len lt do
      len str rot Lexer.skip-whitespace
      let start do
        len str start Lexer.skip-word
        let cur do
          Tokens.Word      // kind
          str start ptr+   // ptr
          cur start sub    // int
          Lexer.push-token

          cur
        end
      end
    end drop
  end
end

memory stats_buffer 144 end
proc main
  0 O_RDONLY c"test.stck" openat // it tokenizes itself!
  stats_buffer over fstat drop
  stats_buffer 48 ptr+ read64  // offset to st_size
  // offset fd flags prot length addr
  2dup 0 rot rot 2 1 rot 0 sys_mmap syscall6
  cast(ptr)

  let fd size contents do
    size contents Lexer.tokenize

    "Read " puts token-count read64 putu " tokens\n" puts

    token-count read64 0
    while 2dup gt do
      tokens over sizeof(Token) mul ptr+ @Token

      let kind data-ptr data-int do
        "- "                   puts
        kind format-token-kind puts
        " \""                  puts
        data-int data-ptr      puts
        "\" @ "                puts

        data-ptr contents sub cast(int)
        dup               putu
        ".."              puts
        data-int add      putu
        "\n"              puts
      end

      1 add
    end drop drop

    size contents sys_munmap syscall2 drop
    fd close drop
  end
end